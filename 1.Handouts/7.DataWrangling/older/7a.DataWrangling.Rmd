---
title: '7E\. Data Wrangling'
author: 'Z620: Quantitative Biodiversity, Indiana University'
header-includes:
   - \usepackage{array}
output: pdf_document
geometry: margin=2.54cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## DATA WRANGLING OVERVIEW

To show examples of ways to incorporate alpha and beta diversity analyzes with data wrangling, we will use simulated data to show ways to analyze gummy data collected.

### 1) DATA WRANGLING

So, what do we mean by **data wrangling**?
Much of the power of computing stems from the efficiency of performing small tasks in an iterative manner. 
As part of this lesson, we will explore different ways of manipulating or "wrangling" data with the goal of making it easier to perform relevant statistics and generate figures. 
Specifically, we will introduce traditional **control structures** (e.g., for loops), along with alternative and commonly used tools that are unique to the R statistical environment, such as `apply` functions and `tidyverse` packages. 
We will learn about these tools while also gaining exposure to R packages that allow us to simulate and sample biodiversity. 

### 2) SETUP
#### A. Retrieve and set up your working directory

```{r, results = 'hide'}
rm(list=ls()) 
getwd() 
setwd("~/GitHub/QB-2021/1.Handouts/7.ControlStructures")
```

```{r}
package.list <- c('mobsim','vegan','tidyverse','ggplot2','dplyr','broom')
for (package in package.list) {
  if (!require(package, character.only=TRUE, quietly=TRUE)) {
    install.packages(package)
  }
  library(c(package), character.only=TRUE)
}
```


#### B. Simulate and sample data 

Here we will simulate source communities and sample individuals using `mobsim` package to demonstrate a couple of examples for data wrangling. The simulated individuals will mirror what we are doing in the exercise with gummies. Feel free to experiment on your own. 

The following code simulates at containing 1000 individuals (*N*) belonging to 50 species (*S*). 
These individuals are drawn from source community with a log-normal *species abundance distribution (SAD)* with mean = 2 and standard deviation = 0 (even) and standard deviation = 1 (uneven) communities. 

```{r}

#The following code simulates at containing 1000 individuals (*N*) belonging to 50 species (*S*). 
#These individuals are drawn from source community with a log-normal *species abundance distribution (SAD)* with mean = 2 and standard deviation = 0 (even) and standard deviation = 1 (uneven) communities. 

# simulate an even local community
com1 <- sim_poisson_community(s_pool = 50, n_sim = 1000, sad_type = "lnorm", 
        sad_coef = list("meanlog" = 2, "sdlog" = 0))
# visualize the even local community
plot(com1)

# simulate an uneven local community
com2 <- sim_poisson_community(s_pool = 50, n_sim = 1000, sad_type = "lnorm", 
        sad_coef = list("meanlog" = 2, "sdlog" = 1))
# visualize the uneven local community
plot(com2)
```

Let's sample from these source communities as we did in the gummy excersize. 

```{r}
#From the communities we created above, we will now randomly sample 4 different non-overlapping quadrats with area size of 0.08, which represents the proportion of the area sampled in the local community we are interested in. 

com_mat1 <- sample_quadrats(com1, n_quadrats = 4, quadrat_area = 0.08, 
             method = "random", avoid_overlap = T)  


com_mat2 <- sample_quadrats(com2, n_quadrats = 4, quadrat_area = 0.08, 
             method = "random", avoid_overlap = T)  

# Assess the species by site matrix generated automatically from our sampling efforts
print(com_mat1$spec_dat)

print(com_mat2$spec_dat)
```


Remember the `vegan` package that we used to calculate diversity indices. Let's calculate "Shannon entropy" to practice data wrangling. 

```{r}
com1_div <- diversity(com_mat1$spec_dat, index = "invsimpson")
print(com1_div)
com2_div <- diversity(com_mat2$spec_dat, index = "invsimpson")
print(com2_div)
```


##### C. Summarize diversity data with a for loop

Now, we will use control structures, specifically, a for loop, to obtain diversity estimates across different source communities. 
We will calculate the mean and standard error (SE) for each community.

```{r}
# write function for SE equation
SE <- function(x) {
  return(sd(x)/sqrt(length(x)))
}

# The following code binds the two vectors as columns in a data frame 
com_div_all <- t(cbind.data.frame(com1_div, com2_div))

# Create an empty data frame to fill with summary statistics
com_div_sum <- as.data.frame(matrix(ncol = 2 , nrow = 2))
colnames(com_div_sum) <- c("mean", 'se')

# The following for loop will iteratively calculate the mean and standard error for each row, one at a time.
for(i in 1:2) {
  x = as.numeric(com_div_all[i,]) 
  com_div_sum[i,1] <- mean(x) # Calculate the mean and save to diversity indices data
  com_div_sum[i,2] <- SE(x)   # Calculate the standard error and save to diversity indices data
}

# Let's add a grouping column to the data
com_div_sum$community <- c("even", "uneven")
```

Now let's plot mean and standard errors of Shannon entropy across the source communities. In QB, most plotting will be done in using **R base package**. 
However, lots of people like visualizing their data using the package *`ggplot`*. We invite interested students to explore `ggplot2` package. 
Let's use this package to graphically explore differences in diversity between different source communities.

```{r}
ggplot(data = com_div_sum, aes(x = community, y = mean))+
  geom_point(size=4)+
  geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width=.2)+
  ylab("Mean inverse Simpson index")+
  xlab("Source community")+
  theme_bw()
```

##### Summarize diversity data with the `apply` function

Although for loops and other control structures (e.g., while loops, if-then-else) are commonly used by computational biologists as we saw above, there are some downsides to its use.
People argue that they lack clarity and can easily break.
This can create confusion when the goal is for code to be robust and reproducible in collaborative projects. 
Also, in R, control structures can be slow and inefficient. 
Fortunately, there are some alternatives, that we will introduce. 
Within the base R package, there is a family of **`apply`** functions. 
This allows one to apply functions across row and columns of matrices, lists, vectors, and data frames. 
The structure of the `apply` function differs from the for loop. 
In fact, it can be done with a single line of code, but it  has a similar sort of logic. 
First, you specify the data object of interest. 
Second, you specify the margin that the function will operate upon (row =1, column = 2, cell = 1:2). 
Last, you specify the function to use, which can be a native, built-in function (e.g., `mean`) or one that you have written yourself (e.g., `SE`)
Run the following code and compare to findings generated with the for loop above:


```{r}
# if only interested in mean, can just write `mean` at end of function
# for both mean and sem, here, we specify as follows:
com_div_sum_apply <- apply(com_div_all, 1, FUN = function(x) {c(mean = mean(x[x>0]), SE = sd(x[x>0])/sqrt(length(x[x>0])))})
print(com_div_sum_apply)
```

#### E. Measure between sample diversity

We have seen different multivariates statistical approaches to compare biodiversity between samples. Here we will incorporate an ordination to measure the differences between our even and uneven communities.

##### Wrangle data with `tidyverse`

Another popular approach for data wrangling is **`tidyverse`**, which is a collection R packages.
**`dplyr`** contains a set of functions that manipulates data frames.
**`tidyr`** allow one to pivot, nest, and separate data in various ways.
And **`ggplot2`** is an alternative to plotting in base R that was designed to interface with `tidyverse`. 
`tidyverse` uses the **pipe operator**, depicted as `%>%`, to string together a sequence of functions or operations. 
Below, we will use the pipe operator in combination to `shape` and `summarize` our diversity data, before making a plot. 

```{r}
# Wrangle data to plot PCA results
com_mat_tidy <- com_mat1$spec_dat %>% # identify data frame of interest
                bind_rows(com_mat2$spec_dat) %>% # bind with another dataset
                replace(is.na(.), 0) %>% # you can replace NAs with 0 abundance
                mutate_if(is.integer, as.numeric) %>% # basic operations
                prcomp(center = T, scale = T) %>% # run a pca
                tidy('scores') %>% # this is a useful broom function
            # to tidy model outputs as data frames
                filter(PC<3) %>% # filter principle components above 2
                pivot_wider(names_from = PC, values_from = value) %>%
            # this converts the data to wide formate, opposite is pivot_longer
                mutate(site = rep(c('1','2','3','4'), times=2)) %>%
            # add a site column for plotting
                mutate(community = rep(c('even', 'uneven'), each=4)) %>%
            # add a community column for plotting
                rename('PC1' = '1', 'PC2' = '2') %>% # you can rename variable names
                select(community, site, PC1, PC2) 
                
# let's plot PCA results 
ggplot(data = com_mat_tidy, aes(x = PC1, y = PC2, color = site, shape = community))+
  geom_point(size = 5)+
  theme_bw()
```  
